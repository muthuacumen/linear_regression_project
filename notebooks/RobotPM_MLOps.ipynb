{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot Predictive Maintenance - MLOps Workshop\n",
    "\n",
    "## Univariate Linear Regression for Maintenance Prediction\n",
    "\n",
    "This notebook demonstrates a complete machine learning workflow for predictive maintenance using univariate linear regression.\n",
    "\n",
    "### Workshop Objectives\n",
    "1. Define a univariate regression problem for robot maintenance\n",
    "2. Implement linear regression from scratch AND with scikit-learn\n",
    "3. Evaluate and compare models\n",
    "4. Generate maintenance alerts based on predictions\n",
    "5. Apply MLOps best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt 1: Problem Definition (UNIVARIATE)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "**Objective:** Predict robot motor degradation over time using univariate linear regression.\n",
    "\n",
    "**Variables:**\n",
    "- **X (Independent Variable):** `elapsed_hours` - Time elapsed since monitoring began\n",
    "- **y (Dependent Variable):** `mean_current` - Average current draw across all motor axes\n",
    "\n",
    "**Why Univariate?**\n",
    "- Single predictor (time) maps directly to single outcome (current)\n",
    "- Allows clear interpretation: slope = degradation rate (Amps/hour)\n",
    "- Foundation for understanding before multivariate expansion\n",
    "\n",
    "**Maintenance Relevance:**\n",
    "- **Positive slope** indicates increasing current draw = motor degradation\n",
    "- **Slope magnitude** quantifies degradation speed\n",
    "- **Prediction** enables proactive maintenance scheduling\n",
    "\n",
    "### Mathematical Model\n",
    "```\n",
    "mean_current = slope * elapsed_hours + intercept\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `slope` = degradation rate (A/hour)\n",
    "- `intercept` = baseline current at t=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import load_csv, compute_features, get_data_summary\n",
    "from preprocessing import handle_missing_values, scale_features, temporal_train_test_split\n",
    "from model import LinearRegressionScratch, train_sklearn_model, compare_models\n",
    "from evaluation import compute_metrics, print_metrics, plot_regression\n",
    "from alert_system import generate_alerts_for_fleet, print_alerts\n",
    "\n",
    "print(\"Modules imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment configuration\n",
    "with open('../configs/experiment_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Data source: {config['data']['source']}\")\n",
    "print(f\"  Feature (X): {config['model']['feature']}\")\n",
    "print(f\"  Target (y): {config['model']['target']}\")\n",
    "print(f\"  Train/Test split: {config['preprocessing']['train_test_split']['ratio']}\")\n",
    "print(f\"  Learning rate: {config['model']['gradient_descent']['learning_rate']}\")\n",
    "print(f\"  Iterations: {config['model']['gradient_descent']['n_iterations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt 2: Data Preprocessing\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "1. **Missing Values:** Forward fill (preserves temporal order)\n",
    "2. **Feature Scaling:** MinMax normalization to [0,1]\n",
    "3. **Train/Test Split:** Temporal (80/20) - no future leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = config['data']['csv_path']\n",
    "df = load_csv(data_path)\n",
    "\n",
    "# Data summary\n",
    "summary = get_data_summary(df)\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  Records: {summary['record_count']:,}\")\n",
    "print(f\"  Axes: {summary['axis_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features\n",
    "df = compute_features(df)\n",
    "\n",
    "# Handle missing values\n",
    "df = handle_missing_values(df, strategy='forward_fill')\n",
    "\n",
    "# Temporal train/test split\n",
    "train_df, test_df = temporal_train_test_split(df, train_ratio=0.8)\n",
    "\n",
    "print(f\"\\nPreprocessing complete:\")\n",
    "print(f\"  Training samples: {len(train_df):,}\")\n",
    "print(f\"  Test samples: {len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt 3: Linear Regression FROM SCRATCH\n",
    "\n",
    "### Mathematical Components\n",
    "\n",
    "1. **Hypothesis:** h(x) = w*x + b\n",
    "2. **Cost (MSE):** J = (1/2m) * sum((h(x) - y)^2)\n",
    "3. **Gradients:** dw = (1/m) * sum((h(x) - y) * x), db = (1/m) * sum(h(x) - y)\n",
    "4. **Update:** w := w - alpha * dw, b := b - alpha * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "feature = config['model']['feature']\n",
    "target = config['model']['target']\n",
    "\n",
    "X_train = train_df[feature].values\n",
    "y_train = train_df[target].values\n",
    "X_test = test_df[feature].values\n",
    "y_test = test_df[target].values\n",
    "\n",
    "print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test data shape: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train from-scratch model\n",
    "lr = config['model']['gradient_descent']['learning_rate']\n",
    "n_iter = config['model']['gradient_descent']['n_iterations']\n",
    "\n",
    "scratch_model = LinearRegressionScratch(learning_rate=lr, n_iterations=n_iter)\n",
    "scratch_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_train_scratch = scratch_model.predict(X_train)\n",
    "y_pred_test_scratch = scratch_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt 4: Linear Regression with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sklearn model\n",
    "sklearn_model, sklearn_coef = train_sklearn_model(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_train_sklearn = sklearn_model.predict(X_train.reshape(-1, 1))\n",
    "y_pred_test_sklearn = sklearn_model.predict(X_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison = compare_models(scratch_model, sklearn_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt 5: Model Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate from-scratch model\n",
    "metrics_scratch = compute_metrics(y_test, y_pred_test_scratch)\n",
    "print_metrics(metrics_scratch, \"From-Scratch Model\")\n",
    "\n",
    "# Evaluate sklearn model\n",
    "metrics_sklearn = compute_metrics(y_test, y_pred_test_sklearn)\n",
    "print_metrics(metrics_sklearn, \"Scikit-learn Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression\n",
    "fig = plot_regression(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    y_pred_train_scratch, y_pred_test_scratch,\n",
    "    model_name=\"Univariate Linear Regression\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Prompt 12: Failure Prediction & Alerts (Fleet-Wide)\n\nThis cell processes each robot individually:\n1. Trains a separate regression model per robot\n2. Computes degradation slope for each\n3. Generates maintenance alerts for the entire fleet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare regression results for ALL robots in the fleet\n# First, check if data has robot_id column and process each robot separately\n\nif 'robot_id' in df.columns:\n    robot_ids = df['robot_id'].unique()\n    print(f\"Found {len(robot_ids)} robots: {list(robot_ids)}\")\n    \n    # Store models and results per robot\n    robot_models = {}\n    regression_results = {}\n    \n    for robot_id in robot_ids:\n        # Filter data for this robot\n        robot_df = df[df['robot_id'] == robot_id].copy()\n        \n        # Recompute elapsed_hours relative to this robot's start time\n        time_col = 'Time' if 'Time' in robot_df.columns else 'timestamp'\n        robot_df['elapsed_hours'] = (robot_df[time_col] - robot_df[time_col].min()).dt.total_seconds() / 3600\n        \n        # Split for this robot\n        robot_train, robot_test = temporal_train_test_split(robot_df, train_ratio=0.8)\n        \n        X_train_r = robot_train[feature].values\n        y_train_r = robot_train[target].values\n        X_test_r = robot_test[feature].values\n        \n        # Train model for this robot\n        model = LinearRegressionScratch(learning_rate=lr, n_iterations=n_iter)\n        model.fit(X_train_r, y_train_r)\n        \n        # Store model and results\n        robot_models[robot_id] = {\n            'model': model,\n            'X_train': X_train_r,\n            'y_train': y_train_r,\n            'X_test': X_test_r,\n            'y_test': robot_test[target].values\n        }\n        \n        regression_results[robot_id] = {\n            'slope': model.weight,\n            'intercept': model.bias,\n            'current_hours': X_test_r.max(),\n            'baseline_value': model.bias\n        }\n        \n        print(f\"  {robot_id}: slope={model.weight:.6f}, intercept={model.bias:.4f}\")\nelse:\n    # Fallback for single robot data without robot_id column\n    robot_ids = ['Robot_A']\n    robot_models = {\n        'Robot_A': {\n            'model': scratch_model,\n            'X_train': X_train,\n            'y_train': y_train,\n            'X_test': X_test,\n            'y_test': y_test\n        }\n    }\n    regression_results = {\n        'Robot_A': {\n            'slope': scratch_model.weight,\n            'intercept': scratch_model.bias,\n            'current_hours': X_test.max(),\n            'baseline_value': scratch_model.bias\n        }\n    }\n\n# Generate alerts for the entire fleet\nalerts = generate_alerts_for_fleet(regression_results)\nprint_alerts(alerts)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Prompt 14: Experiment Tracking (Fleet-Wide)\n\nLog results for ALL robots to experiments/results.csv for auditability.\nEach robot gets its own row with metrics, slope, and alert status."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Log experiment results for ALL robots\nexperiment_id = f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\nexperiment_results = []\n\n# Create a mapping of robot_id to their alerts\nalert_map = {alert.robot_id: alert for alert in alerts}\n\nfor robot_id in robot_ids:\n    robot_data = robot_models[robot_id]\n    model = robot_data['model']\n    \n    # Compute metrics for this robot\n    y_pred_test = model.predict(robot_data['X_test'])\n    robot_metrics = compute_metrics(robot_data['y_test'], y_pred_test)\n    \n    # Get alert for this robot\n    robot_alert = alert_map.get(robot_id)\n    \n    experiment_result = {\n        'timestamp': datetime.now().isoformat(),\n        'experiment_id': experiment_id,\n        'robot_id': robot_id,\n        'feature': feature,\n        'target': target,\n        'model_version': '1.0',\n        'train_samples': len(robot_data['X_train']),\n        'test_samples': len(robot_data['X_test']),\n        'rmse': robot_metrics.rmse,\n        'mae': robot_metrics.mae,\n        'r2': robot_metrics.r2,\n        'slope': model.weight,\n        'intercept': model.bias,\n        'alert_level': robot_alert.level.value if robot_alert else 'NONE',\n        'alert_threshold_crossed': robot_alert.level.value != 'NONE' if robot_alert else False,\n        'notes': 'Fleet-wide experiment'\n    }\n    experiment_results.append(experiment_result)\n    print(f\"Logged: {robot_id} - RMSE: {robot_metrics.rmse:.4f}, R2: {robot_metrics.r2:.4f}, Alert: {experiment_result['alert_level']}\")\n\n# Append all results to file\nresults_df = pd.DataFrame(experiment_results)\nresults_df.to_csv('../experiments/results.csv', mode='a', header=False, index=False)\nprint(f\"\\nExperiment {experiment_id} logged to results.csv ({len(experiment_results)} robots)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\n### What We Accomplished\n1. Defined univariate regression problem (elapsed_hours -> mean_current)\n2. Implemented preprocessing pipeline (missing values, scaling, temporal split)\n3. Built linear regression from scratch using gradient descent\n4. Compared with scikit-learn implementation\n5. Evaluated models using RMSE, MAE, R2\n6. **Fleet-wide processing:** Trained individual models per robot (Robot_A, Robot_B, Robot_C, Robot_D)\n7. Generated maintenance alerts for the entire fleet with severity levels\n8. Logged experiment results for all robots to enable fleet-wide tracking\n\n### MLOps Principles Applied\n- **Separation of concerns:** Modular code in src/\n- **Configuration-driven:** YAML configs control experiments\n- **Reproducibility:** Fixed seeds, versioned data, logged parameters\n- **Experiment tracking:** Results logged to CSV (one row per robot per experiment)\n- **Auditability:** Full lineage from data to alerts\n- **Fleet management:** Individual models per robot for accurate per-unit monitoring"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}